\section{Ejercicio 2}

\subsection{Introducción}
Este ejercicio consistio en crear un modelo de mapeo de caracteristicas auto-organizadas con el objetivo de clasificar documentos. El mapa
auto-organizado que se utilizo fue una grilla bidimensional de 10 filas y 10 columnas. El objetivo de este ejercicio es el de observar espacialamente
en la grilla las distintas clases de los datos.

Para el entrenamiento se utilizo la siguiente formula para obtener la neurona ganadora $k^*$:
  \[
  k^* = \argmin_{i,j} \lvert\lvert x-w_{i,j} \rvert\rvert
  \]

La regla de entrenamiento en cada patron luego de obtener la neurona ganadora es

\begin{equation}
	\Delta W = \eta_t \cdot h_{k^*}(i,j) \cdot (X^{\mu}-W_{i,j})
\end{equation}
donde h es una función que mapea la distancia entre dos neuronas
según la distribución normal.
\[
	h_{k^*}(i, j) = e^{-\frac{d_{k^*}(i,j)^2}{2}}
\]
con $d_{k^*}(i,j)$ es la típica distancia euclideana entre los puntos de la grilla (i,j) y
la neurona ganadora $k^*$ escalada por un factor de $1/\sigma_t$, ie.
$ d_{k^*}(i,j) = \frac{\lvert \lvert (i,j)-k^* \rvert \rvert}{\sigma_t} $

Las funciones de enfriamiento de $\eta$ y $\sigma$ que se utilizaron
en cada iteración $t$ fueron las siguientes:
\[
  \begin{array}{ccc}
    \eta_t & = & \eta_0 \cdot e^{\frac{-x}{\tau_1}} \\
    \sigma_t & = & \sigma_0 \cdot e^{\frac{-x}{\tau_2}} \\
  \end{array}
\]

Este enfriamiento de las variables $\eta$ y $\sigma$ permiten ir disminuyendo
el nivel de aprendizaje y el area de vecindad respectivamente de cada neurona
para que la red a traves del tiempo pueda converger a un estado final estable.


\subsection{Resultados}
\subsubsection{Elección de los parametros}
\begin{itemize}
	\item $\eta_0$: debe eligirse grande de forma tal que al inicio
puedan ser realizados cambios bruscos por la red para
poder organizarse inicialmente.

	\item $\sigma_0$: Inicialmente tiene que ser grande para que el área
de vecindad pueda abarcar a todos los nodos con esto alcanzaría aproximadamente
con el tamaño del díametro de la grilla

\[
	diam = \sqrt{rows^2+cols^2}
\]
	\item $\tau_1$ y $\tau_2$ deben elegirse de forma tal que al terminar
	todas las iteraciones los valores de $\eta$ y $\sigma$ alcancen
	una pequeña proporcion de los $\eta$ y $\sigma$ iniciales.

	La cantidad de iteraciones totales es $epochs \cdot training\_size $.
	Entonces si se quiere al final del entrenamiento que $\eta(t) \in (\eta_{f_l}, \eta_{f_u})$.
	Debe cumplirse que

	\[ \frac{t}{ln(\frac{\eta_0}{\eta_{f_l}})} < \tau_{1} < \frac{t}{ln(\frac{\eta_0}{\eta_{f_u}})} \]

	De igual manera si se quiere que $\sigma(t) \in (\eta_{f_l}, \eta_{f_u})$. Debe cumplirse que

	\[ \frac{t}{ln(\frac{\sigma_0}{\sigma_{f_l}})} < \tau_{2} < \frac{t}{ln(\frac{\sigma_0}{\sigma_{f_u}})} \]

	En las experimentaciones con las formulas anteriores se buscó que el $\eta$ final este entre 0.001 y 0.002.
	y que el $\sigma$ final termine entre 0.1 y 0.05.

\end{itemize}
Tambien se decidio experimentar preprocesando la entrada de la red neuronal para reducir la dimensionalidad de esta.
 Para esto se entreno una red tal como fue explicada en el ejercicio anterior la cual redujo la dimensionalidad proyectando
 a 3 componentes principales en algunos casos y 9 en otros.

Por lo anteriormente explicado para el entrenamiento se utilizaron los siguientes parametros:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% AGREGAR PARAMETROS
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    filas & columnas & epochs & $\eta_0$ & $\sigma_0$ & $\tau_1$ & $\tau_2$ & preprocess & componentes \\
    \hline
	10 & 10 & 100 & 0.1 & 16 & 1300 & 1100 & NO & \ \\
    \hline
	10 & 10 & 100 & 0.1 & 16 & 1300 & 1100 & SI & 3 \\
    \hline
	10 & 10 & 100 & 0.1 & 16 & 1300 & 1100 & SI & 9 \\
    \hline
	25 & 25 & 100 & 0.1 & 36 & 1300 & 1100 & NO & \  \\
    \hline
	25 & 25 & 100 & 0.1 & 36 & 1300 & 1100 & SI & 9  \\
    \hline
  25 & 25 & 100 & 0.1 & 36 & 1300 & 1100 & SI & 3  \\
    \hline
  \end{tabular}
\end{center}

Para la visualizacion de los resultados se realizo un grafico que por cada
muestra del conjunto de entrenamiento con su correspondiente etiqueta, se
calculo cual fue la neurona ganadora. Luego se calculo por cada neurona cual
fue la etiqueta que mas la activo y se la coloreo en base a esa categoria. Cabe destacar que
las neuronas que nunca fueron activadas por ninguna categoria fueron coloreadas con el color gris.
Los resultados obtenidos fueron los siguientes

\begin{figure}[H]
  \includegraphics[width=160mm]{imagenes/som_10_10.png}
  \caption{Grilla de 10x10 sin preprocesamiento}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=160mm]{imagenes/som_10_10_3_preprocess.png}
  \caption{Grilla de 10x10 con preprocesamiento y 3 componentes principales}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=160mm]{imagenes/som_10_10_9_preprocess.png}
  \caption{Grilla de 10x10 con preprocesamiento y 9 componentes principales}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=160mm]{imagenes/som_25_25.png}
  \caption{Grilla de 25x25 sin preprocesamiento}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=160mm]{imagenes/som_25_25_3_preprocess.png}
  \caption{Grilla de 25x25 con preprocesamiento y 3 componentes principales}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=160mm]{imagenes/som_25_25_9_preprocess.png}
  \caption{Grilla de 25x25 con preprocesamiento y 9 componentes principales}
\end{figure}

\subsection{Conclusión}
Una conclusion que se observo fue la diferencia en los tiempos de entrenamiento de la red con preprocesamiento
con respecto al entrenamiento sin preprocesado. Esto tiene sentido ya que se esta entrenando con una entrada de menor
dimensionalidad logrando que el computo realizado sea menor. Con respecto a la calidad obtenida por cada experimento
se concluyo que la calidad de la red con preprocesamiento es superior a la red sin preprocesamiento, considerando calidad
la forma en la que la red agrupa instancias.
